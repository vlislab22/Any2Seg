<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <title>Any2Seg</title>
    <meta name="author" content="Xu Zheng">
    <meta name="description" content="Project page of MAGIC">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link rel="icon" type="image/png" href="eccv_logo.png">
    
    <!-- 样式 -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="../format/app.css">
    <link rel="stylesheet" href="../format/bootstrap.min.css">
    
    <!-- 脚本 -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    <script src="../format/app.js"></script>
  </head>

  <body style="text-align: center;">
    <div class="container" id="main">
        <div class="row">
            <h1 class="col-md-12 text-center">
                Centering the Value of Every Modality: Towards Efficient and Resilient Modality-agnostic Semantic Segmentation<br /> 
                <small>
                    ECCV 2024
                </small>
            </h1>
        </div>
        
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <img src="./images/xu1.png" height="80px"><br>
                        <a href="https://zhengxujosh.github.io/">Xu Zheng</a><br />
                        AI Thrust, HKUST(GZ)<br /> &nbsp &nbsp
                    </li>
                    <li>
                        <img src="./images/huiyi1.png" height="80px"><br>
                        <a href="https://qc-ly.github.io/">Yuanhuiyi Lyu</a><br />
                        AI Thrust, HKUST(GZ)<br /> &nbsp &nbsp
                    </li>
                    <li>
                        <img src="./images/Addision.png" height="80px"><br>
                        <a href="https://addisonwang2013.github.io/vlislab/linwang.html">Addison Lin Wang</a><br />
                        AI & CMA Thrust, HKUST(GZ)<br /> Dept. of CSE, HKUST
                    </li>
                </ul>
            </div>
        </div>

        <!-- ##### 元素 #####-->
        <div class="row">
            <div class="col-md-8 col-md-offset-2 text-center">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="">
                            <img src="./images/arxiv.png" height="100px"><br>
                            <h4><strong>Paper</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="">
                            <img src="./images/github_icon.jpg" height="100px"><br>
                            <h4><strong>Code</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="https://vlislab22.github.io/vlislab/">
                            <img src="./images/lab_logo.png" height="100px"><br>
                            <h4><strong>Vlislab</strong></h4>
                        </a>
                    </li>                       
                </ul>
            </div>
        </div>

        <!-- ##### 摘要 #####-->
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>Abstract</h3>
                <p class="text-justify">
                    Image modality is not perfect as it often fails in certain conditions, \eg, night and fast motion. 
                  This significantly limits the robustness and versatility of existing multi-modal (\ie, Image+X) semantic segmentation methods when confronting modality absence or failure, as often occurred in real-world applications. 
                  Inspired by the open-world learning capability of multi-modal vision-language models (MVLMs), we explore a new direction in learning the modality-agnostic representation via knowledge distillation (KD) from MVLMs. 
                  Intuitively, we propose \textbf{\textit{Any2Seg}}, a novel framework that can achieve robust segmentation from \textbf{\textit{any}} combination of modalities in \textbf{\textit{any}} visual conditions. 
                  Specifically, we first introduce a novel language-guided semantic correlation distillation (\textbf{LSCD}) module to transfer both inter-modal and intra-modal semantic knowledge in the embedding space from MVLMs, \eg, LanguageBind~\cite{zhu2023languagebind}. 
                  This enables us to minimize the modality gap and alleviate semantic ambiguity to combine any modalities in any visual conditions. Then, we introduce a modality-agnostic feature fusion (\textbf{MFF}) module that reweights the multi-modal features based on the inter-modal correlation and selects the fine-grained feature. 
                  This way, our Any2Seg finally yields an optimal modality-agnostic representation. 
                  Extensive experiments on two benchmarks with four modalities demonstrate that Any2Seg achieves the state-of-the-art under the multi-modal setting (\textbf{+3.54} mIoU) and excels in the challenging modality-incomplete setting(\textbf{+19.79} mIoU).  
                </p>
            </div>
        </div>
      
        <div class="container">
            <h1>Demo Video</h1>
            
            <!-- 视频标签 -->
            <!-- 使用 video 标签嵌入视频 -->
            <video controls>
                <!-- 替换为您的视频文件路径 -->
                <source src="./images/4978_vid.mp4" type="video/mp4">
            </video>
            
            <!-- 视频说明 -->
            <p>Here is a demo video for the proposed MAGIC.</p>
        </div>

        <!-- ##### 结果 #####-->
        <div class="row">     
            <div class="col-md-8 col-md-offset-2">
                <h3>Overall framework of our MAGIC</h3>
                <p class="text-justify">
                    Overall framework of our Any2Seg framework.
                </p>
                <img src="./images/Any2Seg_overall.jpg" class="img-responsive" alt="vis_res" class="center">
            </div>
        </div>
      
      <div class="row">     
            <div class="col-md-8 col-md-offset-2">
                <h3>Results</h3>
                <p class="text-justify">
                    Visualization of arbitrary inputs using {RGB, Depth, Event, LiDAR} on DELIVER.
                </p>
                <img src="./images/vis_compare.jpg" class="img-responsive" alt="vis_res" class="center">
            </div>
        </div>

        <!-- ##### BibTex #####-->
        <hr>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>BibTeX</h3>
                <div class="row align-items-center">
                    <div class="col py-3">
                        <pre class="border" style="text-align: left;">
@article{zheng2024MAGIC,
  title={Centering the Value of Every Modality: Towards Efficient and Resilient Modality-agnostic Semantic Segmentation},
  author={Zheng, Xu and Lyu, Yuanhuiyi and Zhou, Jiazhou and Wang, Lin},
  journal={ECCV},
  year={2024}
}
                        </pre>
                    </div>
                </div>
            </div>
        </div>
    </div>
  </body>
</html>
